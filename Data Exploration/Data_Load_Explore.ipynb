{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "\n",
    "When downloading our data, it did not come with headers. We fixed this by adding getting the ordered headers from directory we downloaded the data frame, and implemented then BY USING... From this group of files, we confirmed our theories that variable_name_SE was the standard error and variable_name_Worst was the largest value. Along with the leading diagonal of our pair plots of all the unique variables following a normal distributions, inline with the Central Limit Theorem, gave us confidence we have assigned the correct headers onto our dataframe.\n",
    "\n",
    "Originally our data was in .name and .data file formats, which we could not access. We fixed this by converting the files into a .csv format. This gave us an error that we might lose information, however according to the data set summary we still have every instance of our data and every column, and it is to the same 4 significant figures-so we know no data has been lost.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./wdbc.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean the data\n",
    "\n",
    "Our data is supposedly clean, and observing the first few instances it does seem so. We already know that Pandas only allows one data type per column, so this is a non issue. We will look for duplicates in our data, as the probability of so many factors being identical is significantly low- if we have duplicates this may be a fault in the data collection or the previously mentioned .csv method of extracting the data. One issue that we cannot determine just from looking at the data, is the data type of our values, i.e a string for a float."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Same patient entered more than once: {df.duplicated().any()}\")\n",
    "df_dup = df.drop('ID', axis=1)\n",
    "print(f\"Same patient entered more than once under a different ID: {df_dup.duplicated().any()}\") # testing with ID\n",
    "print(f\"Null values? {df.isnull().values.any()}\")\n",
    "print(f\"There are {len(df)} instances, of which {df['ID'].nunique()} are unique \\n\")\n",
    "df.info()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will next split our data in its label and features, and drop our ID column- as it bears no effect on our model (and as each instance value overall is unique, we could always reverse engineer our way back to get the ID). As seen above, our B/M column has an object type - this can cause issues with Seaborn plot so we will encode it 0 or 1,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = LabelEncoder().fit(df[\"B/M\"])\n",
    "df['B/M'] = encoder.transform(df[\"B/M\"])\n",
    "label = df[\"B/M\"] #One line here is redundant?\n",
    "not_data = [\"ID\", \"B/M\"]\n",
    "feature = df.drop(not_data, axis=1)\n",
    "data = pd.concat([label, feature], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also preemptively split our data into the train/test/validation sets, however these ratios be tuned at a later date depending on the number of features in our data and the specific algorithm. We used a stratified sample, as this is a classification problem with our labels being either Benign(0) or Malignant(1). We do not have to shuffle our dataframe, as train_test_split(shuffle=True).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(feature, label, test_size=0.25, stratify=label)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.25, stratify=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualization & Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(data=feature, x=label)\n",
    "plt.title(\"Diagnosis\")\n",
    "plt.show()\n",
    "# Make graphs nice\n",
    "sum(label) # 1 mean malignant\n",
    "len(label)\n",
    "print(f\"Percentage of Malignant Tumours is {round(sum(label) / len(label), 3)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ratio of Benign is slightly higher than what we would expect (as ~80% of breast cancer on average is Benign), so our data is not too imbalanced. However, we may still decide to try penalized models as a) percentage of Malignant tumours is higher than normal (so we would penalize malignant) and b) the cost of false negatives is very high (as treatment will be delayed) so we may want to significantly penalize false negatives in our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(data=feature.iloc[::, :10])\n",
    "plt.show()\n",
    "# can't load for me, also how can I add hue to this ('B/M')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "\n",
    "sns.heatmap((data.iloc[::, :10]).corr(), annot=True, fmt=\".1f\", ax=ax)\n",
    "plt.title(\"Feature Correlation\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look at the correlations between B/M and other varaibles, i.e symmetry is only 0.3 so should we remove this vairable?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
