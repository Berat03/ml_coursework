{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "\n",
    "When downloading our data, it did not come with headers. We fixed this by adding getting the ordered headers from directory we downloaded the data frame, and implemented then BY USING... From this group of files, we confirmed our theories that variable_name_SE was the standard error and variable_name_Worst was the largest value. Along with the leading diagonal of our pair plots of all the unique variables following a normal distributions, inline with the Central Limit Theorem, gave us confidence we have assigned the correct headers onto our dataframe.\n",
    "\n",
    "Originally our data was in .name and .data file formats, which we could not access. We fixed this by converting the files into a .csv format. This gave us an error that we might lose information, however according to the data set summary we still have every instance of our data and every column, and it is to the same 4 significant figures-so we know no data has been lost.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "         ID B/M  Radius  Texture  Perimeter    Area  Smoothness  Compactness  \\\n0    842302   M   17.99    10.38     122.80  1001.0     0.11840      0.27760   \n1    842517   M   20.57    17.77     132.90  1326.0     0.08474      0.07864   \n2  84300903   M   19.69    21.25     130.00  1203.0     0.10960      0.15990   \n3  84348301   M   11.42    20.38      77.58   386.1     0.14250      0.28390   \n4  84358402   M   20.29    14.34     135.10  1297.0     0.10030      0.13280   \n5    843786   M   12.45    15.70      82.57   477.1     0.12780      0.17000   \n6    844359   M   18.25    19.98     119.60  1040.0     0.09463      0.10900   \n7  84458202   M   13.71    20.83      90.20   577.9     0.11890      0.16450   \n8    844981   M   13.00    21.82      87.50   519.8     0.12730      0.19320   \n9  84501001   M   12.46    24.04      83.97   475.9     0.11860      0.23960   \n\n   Concavity  ConcavePoints  ...  RadiusWorst  TextureWorst  PerimeterWorst  \\\n0    0.30010        0.14710  ...        25.38         17.33          184.60   \n1    0.08690        0.07017  ...        24.99         23.41          158.80   \n2    0.19740        0.12790  ...        23.57         25.53          152.50   \n3    0.24140        0.10520  ...        14.91         26.50           98.87   \n4    0.19800        0.10430  ...        22.54         16.67          152.20   \n5    0.15780        0.08089  ...        15.47         23.75          103.40   \n6    0.11270        0.07400  ...        22.88         27.66          153.20   \n7    0.09366        0.05985  ...        17.06         28.14          110.60   \n8    0.18590        0.09353  ...        15.49         30.73          106.20   \n9    0.22730        0.08543  ...        15.09         40.68           97.65   \n\n   AreaWorst  SmoothnessWorst  CompactnessWorst  ConcavityWorst  \\\n0     2019.0           0.1622            0.6656          0.7119   \n1     1956.0           0.1238            0.1866          0.2416   \n2     1709.0           0.1444            0.4245          0.4504   \n3      567.7           0.2098            0.8663          0.6869   \n4     1575.0           0.1374            0.2050          0.4000   \n5      741.6           0.1791            0.5249          0.5355   \n6     1606.0           0.1442            0.2576          0.3784   \n7      897.0           0.1654            0.3682          0.2678   \n8      739.3           0.1703            0.5401          0.5390   \n9      711.4           0.1853            1.0580          1.1050   \n\n   ConcavePointsWorst  SymmetryWorst  FractalDimensionWorst  \n0              0.2654         0.4601                0.11890  \n1              0.1860         0.2750                0.08902  \n2              0.2430         0.3613                0.08758  \n3              0.2575         0.6638                0.17300  \n4              0.1625         0.2364                0.07678  \n5              0.1741         0.3985                0.12440  \n6              0.1932         0.3063                0.08368  \n7              0.1556         0.3196                0.11510  \n8              0.2060         0.4378                0.10720  \n9              0.2210         0.4366                0.20750  \n\n[10 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>B/M</th>\n      <th>Radius</th>\n      <th>Texture</th>\n      <th>Perimeter</th>\n      <th>Area</th>\n      <th>Smoothness</th>\n      <th>Compactness</th>\n      <th>Concavity</th>\n      <th>ConcavePoints</th>\n      <th>...</th>\n      <th>RadiusWorst</th>\n      <th>TextureWorst</th>\n      <th>PerimeterWorst</th>\n      <th>AreaWorst</th>\n      <th>SmoothnessWorst</th>\n      <th>CompactnessWorst</th>\n      <th>ConcavityWorst</th>\n      <th>ConcavePointsWorst</th>\n      <th>SymmetryWorst</th>\n      <th>FractalDimensionWorst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>843786</td>\n      <td>M</td>\n      <td>12.45</td>\n      <td>15.70</td>\n      <td>82.57</td>\n      <td>477.1</td>\n      <td>0.12780</td>\n      <td>0.17000</td>\n      <td>0.15780</td>\n      <td>0.08089</td>\n      <td>...</td>\n      <td>15.47</td>\n      <td>23.75</td>\n      <td>103.40</td>\n      <td>741.6</td>\n      <td>0.1791</td>\n      <td>0.5249</td>\n      <td>0.5355</td>\n      <td>0.1741</td>\n      <td>0.3985</td>\n      <td>0.12440</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>844359</td>\n      <td>M</td>\n      <td>18.25</td>\n      <td>19.98</td>\n      <td>119.60</td>\n      <td>1040.0</td>\n      <td>0.09463</td>\n      <td>0.10900</td>\n      <td>0.11270</td>\n      <td>0.07400</td>\n      <td>...</td>\n      <td>22.88</td>\n      <td>27.66</td>\n      <td>153.20</td>\n      <td>1606.0</td>\n      <td>0.1442</td>\n      <td>0.2576</td>\n      <td>0.3784</td>\n      <td>0.1932</td>\n      <td>0.3063</td>\n      <td>0.08368</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>84458202</td>\n      <td>M</td>\n      <td>13.71</td>\n      <td>20.83</td>\n      <td>90.20</td>\n      <td>577.9</td>\n      <td>0.11890</td>\n      <td>0.16450</td>\n      <td>0.09366</td>\n      <td>0.05985</td>\n      <td>...</td>\n      <td>17.06</td>\n      <td>28.14</td>\n      <td>110.60</td>\n      <td>897.0</td>\n      <td>0.1654</td>\n      <td>0.3682</td>\n      <td>0.2678</td>\n      <td>0.1556</td>\n      <td>0.3196</td>\n      <td>0.11510</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>844981</td>\n      <td>M</td>\n      <td>13.00</td>\n      <td>21.82</td>\n      <td>87.50</td>\n      <td>519.8</td>\n      <td>0.12730</td>\n      <td>0.19320</td>\n      <td>0.18590</td>\n      <td>0.09353</td>\n      <td>...</td>\n      <td>15.49</td>\n      <td>30.73</td>\n      <td>106.20</td>\n      <td>739.3</td>\n      <td>0.1703</td>\n      <td>0.5401</td>\n      <td>0.5390</td>\n      <td>0.2060</td>\n      <td>0.4378</td>\n      <td>0.10720</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>84501001</td>\n      <td>M</td>\n      <td>12.46</td>\n      <td>24.04</td>\n      <td>83.97</td>\n      <td>475.9</td>\n      <td>0.11860</td>\n      <td>0.23960</td>\n      <td>0.22730</td>\n      <td>0.08543</td>\n      <td>...</td>\n      <td>15.09</td>\n      <td>40.68</td>\n      <td>97.65</td>\n      <td>711.4</td>\n      <td>0.1853</td>\n      <td>1.0580</td>\n      <td>1.1050</td>\n      <td>0.2210</td>\n      <td>0.4366</td>\n      <td>0.20750</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 32 columns</p>\n</div>"
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./wdbc.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean our data(?)\n",
    "\n",
    "Our data is supposedly clean, and observing the first few instances it does seem so. We already know that Pandas only allows one data type per column, so this is a non issue. We will look for duplicates in our data, as the probability of so many factors being identical is significantly low- if we have duplicates this may be a fault in the data collection or the previously mentioned .csv method of extracting the data. One issue that we cannot determine just from looking at the data, is the data type of our values, i.e a string for a float."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same patient entered more than once: False\n",
      "Same patient entered more than once under a different ID: False\n",
      "Null values? False\n",
      "There are 569 instances, of which 569 are unique \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   ID                     569 non-null    int64  \n",
      " 1   B/M                    569 non-null    object \n",
      " 2   Radius                 569 non-null    float64\n",
      " 3   Texture                569 non-null    float64\n",
      " 4   Perimeter              569 non-null    float64\n",
      " 5   Area                   569 non-null    float64\n",
      " 6   Smoothness             569 non-null    float64\n",
      " 7   Compactness            569 non-null    float64\n",
      " 8   Concavity              569 non-null    float64\n",
      " 9   ConcavePoints          569 non-null    float64\n",
      " 10  Symmetry               569 non-null    float64\n",
      " 11  FractalDimension       569 non-null    float64\n",
      " 12  RadiusSE               569 non-null    float64\n",
      " 13  TextureSE              569 non-null    float64\n",
      " 14  PerimeterSE            569 non-null    float64\n",
      " 15  AreaSE                 569 non-null    float64\n",
      " 16  SmoothnessSE           569 non-null    float64\n",
      " 17  CompactnessSE          569 non-null    float64\n",
      " 18  ConcavitySE            569 non-null    float64\n",
      " 19  ConcavePointsSE        569 non-null    float64\n",
      " 20  SymmetrySE             569 non-null    float64\n",
      " 21  FractalDimensionSE     569 non-null    float64\n",
      " 22  RadiusWorst            569 non-null    float64\n",
      " 23  TextureWorst           569 non-null    float64\n",
      " 24  PerimeterWorst         569 non-null    float64\n",
      " 25  AreaWorst              569 non-null    float64\n",
      " 26  SmoothnessWorst        569 non-null    float64\n",
      " 27  CompactnessWorst       569 non-null    float64\n",
      " 28  ConcavityWorst         569 non-null    float64\n",
      " 29  ConcavePointsWorst     569 non-null    float64\n",
      " 30  SymmetryWorst          569 non-null    float64\n",
      " 31  FractalDimensionWorst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Same patient entered more than once: {df.duplicated().any()}\")\n",
    "df_dup = df.drop('ID', axis=1)\n",
    "print(f\"Same patient entered more than once under a different ID: {df_dup.duplicated().any()}\") # testing with ID\n",
    "print(f\"Null values? {df.isnull().values.any()}\")\n",
    "print(f\"There are {len(df)} instances, of which {df['ID'].nunique()} are unique \\n\")\n",
    "df.info()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will next split our data in its label and features, and drop our ID column- as it bears no effect on our model (and as each instance value overall is unique, we could always reverse engineer our way back to get the ID). As seen above, our B/M column has an object type - this can cause issues with Seaborn plot so we will encode it 0 or 1,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "encoder = LabelEncoder().fit(df[\"B/M\"])\n",
    "df['B/M'] = encoder.transform(df[\"B/M\"])\n",
    "label = df[\"B/M\"] #LABEL IS REDUNDANT???\n",
    "not_data = [\"ID\", \"B/M\"]\n",
    "feature = df.drop(not_data, axis=1)\n",
    "data = pd.concat([label, feature], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also preemptively split our data into the train/test/validation sets, however these ratios be tuned at a later date depending on the number of features in our data and the specific algorithm. We used a stratified sample, as this is a classification problem with our labels being either Benign(0) or Malignant(1). We do not have to shuffle our dataframe, as train_test_split(shuffle=True).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(feature, label, test_size=0.25, stratify=label)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.25, stratify=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualization\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
