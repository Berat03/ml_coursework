{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.dummy import DummyClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Get data & split into train/test/validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data Exploration/wdbc.csv')\n",
    "df = df.drop([\"Area\", \"AreaSE\", \"AreaWorst\", \"Perimeter\", \"PerimeterSE\", \"PerimeterWorst\"], axis = 1)\n",
    "encoder = LabelEncoder().fit(df[\"B/M\"])\n",
    "df['B/M'] = encoder.transform(df[\"B/M\"])\n",
    "not_data = [\"ID\", \"B/M\"]\n",
    "label = df[\"B/M\"]\n",
    "feature = df.drop(not_data, axis = 1)\n",
    "data = pd.concat([label, feature], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  1.1 Split into train / validation / test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature, label, test_size=0.3, stratify=label, random_state=2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, stratify=y_train, random_state=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Create & Train Initial Model\n",
    "\n",
    "Our initial model that we train, before hyperparameter tuning. We want a higher Recall, as it accounts for false negatives - and gives us a lower proportion of them, which we desire. We get a very high accuracy and recall and precision - which makes me suspect of overfitting, especially because we have such a small sample size respective to the number of features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98        75\n",
      "           1       0.96      0.98      0.97        45\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.97      0.98      0.97       120\n",
      "weighted avg       0.98      0.97      0.98       120\n",
      "\n",
      "[[73  2]\n",
      " [ 1 44]]\n",
      "Accuracy: 0.975\n",
      "Recall: 0.9777777777777777\n",
      "Precision: 0.9565217391304348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       107\n",
      "           1       0.95      0.86      0.90        64\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.93      0.92      0.92       171\n",
      "weighted avg       0.93      0.93      0.93       171\n",
      "\n",
      "[[104   3]\n",
      " [  9  55]]\n",
      "Accuracy: 0.9298245614035088\n",
      "Recall: 0.859375\n",
      "Precision: 0.9482758620689655\n"
     ]
    }
   ],
   "source": [
    "def bench_marks(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(x_val)\n",
    "bench_marks(y_val, y_pred)\n",
    "\n",
    "y_pred_real = rfc.predict(x_test)\n",
    "bench_marks(y_test, y_pred_real)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Feature Selection\n",
    "Determine importance of each feature, and use recursive feature elimination to determine how many of our features to keep. Then adjust our feature set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConcavePointsWorst       0.182892\n",
      "RadiusWorst              0.151501\n",
      "ConcavePoints            0.142386\n",
      "Radius                   0.093722\n",
      "ConcavityWorst           0.088303\n",
      "Concavity                0.064856\n",
      "RadiusSE                 0.041800\n",
      "Texture                  0.029646\n",
      "TextureWorst             0.028305\n",
      "Compactness              0.026430\n",
      "SymmetryWorst            0.022433\n",
      "CompactnessWorst         0.020030\n",
      "ConcavePointsSE          0.015508\n",
      "SmoothnessWorst          0.012840\n",
      "FractalDimension         0.012239\n",
      "FractalDimensionSE       0.010847\n",
      "FractalDimensionWorst    0.009334\n",
      "ConcavitySE              0.008613\n",
      "TextureSE                0.007362\n",
      "CompactnessSE            0.007099\n",
      "SymmetrySE               0.006742\n",
      "Smoothness               0.006183\n",
      "SmoothnessSE             0.006021\n",
      "Symmetry                 0.004908\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_scores = pd.Series(rfc.feature_importances_, index=x_train.columns).sort_values(ascending=False)\n",
    "print(feature_scores)\n",
    "feature_scores = feature_scores.index.tolist()\n",
    "\n",
    "rfecv = RFECV(estimator=rfc, step=1, cv=5,scoring='accuracy')\n",
    "rfecv = rfecv.fit(x_train, y_train)\n",
    "print(rfecv.n_features_)\n",
    "# will keep these features\n",
    "print(x_train.columns[rfecv.support_])\n",
    "feature_select = feature[x_train.columns[rfecv.support_]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_select, label, test_size=0.3, stratify=label, random_state=2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, stratify=y_train, random_state=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(x_val)\n",
    "bench_marks(y_val, y_pred)\n",
    "\n",
    "y_pred_real = rfc.predict(x_test)\n",
    "bench_marks(y_test, y_pred_real)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Hyperparameter Tuning\n",
    "\n",
    "We will use a random grib with a randomized search to try different iterations of the trees and brute force our way to a optimal solution. We will repeat the code below, adjusting the parameters each iteration. Iterate by running this code while changing values in first box."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = [500, 700, 1000] #Usually more is better and reduce overfitting, but higher computational cost and diminishing returns after ~128\n",
    "max_depth = [5, 6, 7,8, 9, 10, 11, 12,13, 14, 15, 16, 17] # Increase leads to increase in variance and decrease in bias\n",
    "max_features = [\"sqrt\", \"log2\", None]\n",
    "min_samples_split = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "min_samples_leaf = [1, 2, 3, 4, 5, 7]#Increase leads to increase in bias and decrease in variance\n",
    "bootstrap = [True, False]\n",
    "criterion=['gini', 'entropy']\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               #'bootstrap': bootstrap,\n",
    "               'criterion': criterion}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 200, cv = 3, verbose=2, n_jobs = -1, random_state=2)\n",
    "rf_random.fit(x_train, y_train)\n",
    "rfc = rf_random.best_estimator_\n",
    "\n",
    "#ressults with validation set\n",
    "y_pred = rfc.predict(x_val)\n",
    "print(rf_random.best_estimator_)\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_val, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_val, y_pred)}')\n",
    "print(\"\\n\")\n",
    "\n",
    "#results with actual test set\n",
    "y_real_pred = rfc.predict(x_test)\n",
    "print(\"For actual test set we get:\")\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_real_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_real_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_real_pred)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Test for over/ underfitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sizes_abs_accuracy, train_accuracy, test_accuracy = learning_curve(estimator=rfc, X=x_val, y=y_val, scoring='accuracy', cv=5, random_state=2)\n",
    "train_sizes_abs_error, train_error, test_error = learning_curve(estimator=rfc, X=x_val, y=y_val, scoring='recall', cv=5, random_state=0)\n",
    "train_avg_accuracy, test_avg_accuracy, train_avg_error, test_avg_error = [], [], [], []\n",
    "\n",
    "\n",
    "for i in range(len(train_sizes_abs_accuracy)):\n",
    "    train_avg_accuracy.append(np.average(train_accuracy[i]))\n",
    "    test_avg_accuracy.append(np.average(test_accuracy[i]))\n",
    "\n",
    "for i in range(len(train_sizes_abs_error)):\n",
    "    train_avg_error.append(np.average(train_error[i]))\n",
    "    test_avg_error.append(np.average(test_error[i]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Accuracy learning curve\n",
    "ax[0].plot(train_sizes_abs_accuracy, train_avg_accuracy)\n",
    "ax[0].plot(train_sizes_abs_accuracy, test_avg_accuracy)\n",
    "ax[0].legend(['Train', 'Validation'])\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Num. samples')\n",
    "\n",
    "# Error learning curve\n",
    "ax[1].plot(train_sizes_abs_error, train_avg_error)\n",
    "ax[1].plot(train_sizes_abs_error, test_avg_error)\n",
    "ax[1].legend(['Train', 'Validation'])\n",
    "ax[1].set_xlabel('Num. of training samples')\n",
    "ax[1].set_ylabel('Recall')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Could plot against test set, but best to do again validation as it shows us how our model changes.\n",
    "\n",
    "The learning curve changes a lot each time. This may be because there isn't much relationship between the variables and label, but we used cross validation to get rid of this issue - another reason may be because our dataset is too small. There is a fairly large gap between my train and validation sets, so I increased the train/test/validation split as I feel the graph could trend towards a more accurate union between the two sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After hyperparameter tuning* and feature selection*\n",
    "y_real_pred = rfc.predict(x_test)\n",
    "auc = roc_auc_score(y_test, y_real_pred)\n",
    "\n",
    "y_pred_prob = rfc.predict_proba(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, metric in enumerate(list(zip(tpr, fpr, thresholds))[1::]):\n",
    "    tpr_, fpr_, threshold =  metric[0], metric[1], metric[2]\n",
    "    print(f'''\n",
    "    TPR: {tpr_}\n",
    "    FPR: {fpr_}\n",
    "    threshold: {threshold}\n",
    "    threshold_idx: {idx + 1}\n",
    "    ''')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to minimise false positives..\n",
    "\n",
    "How does this graph help us? IG I could manually calculate (using the graph) the precesion, accuracy and  recall stats to determine which point is best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dm = DummyClassifier()\n",
    "dm.fit(x_train, y_train)\n",
    "dummy_score = dm.score(x_test, y_test)\n",
    "y_pred = rfc.predict(x_test)  # Model classifications\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'{\"Our model beats the dummy model\" if accuracy > dummy_score else \"Our model does not beat the dummy model.\"}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_real_pred))\n",
    "c = confusion_matrix(y_test, y_pred_real)\n",
    "print(f'True negatives: {c[0][0]}')\n",
    "print(f'False negatives: {c[1][0]}')\n",
    "print(f'True positives: {c[1][1]}')\n",
    "print(f'False positives: {c[0][1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
