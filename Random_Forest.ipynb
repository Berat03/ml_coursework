{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.dummy import DummyClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Get data & split into train/test/validation sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data Exploration/wdbc.csv')\n",
    "df = df.drop([\"Area\", \"AreaSE\", \"AreaWorst\",\"Perimeter\", \"PerimeterSE\", \"PerimeterWorst\"], axis = 1)\n",
    "encoder = LabelEncoder().fit(df[\"B/M\"])\n",
    "df['B/M'] = encoder.transform(df[\"B/M\"])\n",
    "not_data = [\"ID\", \"B/M\"]\n",
    "label = df[\"B/M\"]\n",
    "feature = df.drop(not_data, axis = 1)\n",
    "data = pd.concat([label, feature], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  1.1 Split into train / validation / test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature, label, test_size=0.25, stratify=label)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Create & Train Initial Model\n",
    "\n",
    "Our initial model that we train, before hyperparameter tuning. We want a higher Recall, as it accounts for false negatives - and gives us a lower proportion of them, which we desire. We get a very high accuracy and recall and precision - which makes me suspect of overfitting, especially because we have such a small sample size respective to the number of features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        67\n",
      "           1       0.90      0.90      0.90        40\n",
      "\n",
      "    accuracy                           0.93       107\n",
      "   macro avg       0.92      0.92      0.92       107\n",
      "weighted avg       0.93      0.93      0.93       107\n",
      "\n",
      "[[63  4]\n",
      " [ 4 36]]\n",
      "Accuracy: 0.9252336448598131\n",
      "Recall: 0.9\n",
      "Precision: 0.9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        90\n",
      "           1       0.94      0.92      0.93        53\n",
      "\n",
      "    accuracy                           0.95       143\n",
      "   macro avg       0.95      0.95      0.95       143\n",
      "weighted avg       0.95      0.95      0.95       143\n",
      "\n",
      "[[87  3]\n",
      " [ 4 49]]\n",
      "Accuracy: 0.951048951048951\n",
      "Recall: 0.9245283018867925\n",
      "Precision: 0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "def bench_marks(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "\n",
    "rfc_og = RandomForestClassifier(n_estimators=100, oob_score=True, bootstrap=True)\n",
    "rfc_og.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rfc_og.predict(x_val)\n",
    "bench_marks(y_val, y_pred)\n",
    "\n",
    "y_pred_real = rfc_og.predict(x_test)\n",
    "bench_marks(y_test, y_pred_real)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Feature Selection\n",
    "Determine importance of each feature, and use recursive feature elimination to determine how many of our features to keep. Then adjust our feature set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rfc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m feature_scores \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(\u001B[43mrfc\u001B[49m\u001B[38;5;241m.\u001B[39mfeature_importances_, index\u001B[38;5;241m=\u001B[39mx_train\u001B[38;5;241m.\u001B[39mcolumns)\u001B[38;5;241m.\u001B[39msort_values(ascending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(feature_scores)\n\u001B[1;32m      3\u001B[0m feature_scores \u001B[38;5;241m=\u001B[39m feature_scores\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mtolist()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'rfc' is not defined"
     ]
    }
   ],
   "source": [
    "feature_scores = pd.Series(rfc.feature_importances_, index=x_train.columns).sort_values(ascending=False)\n",
    "print(feature_scores)\n",
    "feature_scores = feature_scores.index.tolist()\n",
    "\n",
    "rfecv = RFECV(estimator=rfc_og, step=1, cv=5,scoring='accuracy')\n",
    "rfecv = rfecv.fit(x_train, y_train)\n",
    "print(rfecv.n_features_)\n",
    "print(x_train.columns[rfecv.support_])\n",
    "feature_select = feature[x_train.columns[rfecv.support_]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_select, label, test_size=0.25, stratify=label, random_state=2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train, random_state=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(x_val)\n",
    "bench_marks(y_val, y_pred)\n",
    "\n",
    "y_pred_real = rfc.predict(x_test)\n",
    "bench_marks(y_test, y_pred_real)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Hyperparameter Tuning\n",
    "\n",
    "We will use a random grib with a randomized search to try different iterations of the trees and brute force our way to a optimal solution. We will repeat the code below, adjusting the parameters each iteration. Iterate by running this code while changing values in first box."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = [300] #Usually more is better and reduce overfitting, but higher computational cost and diminishing returns after ~128\n",
    "max_depth = [5, 6, 7,8, 9, 10, 11, 12,13, 14, 15, 16, 17] # Increase leads to increase in variance and decrease in bias\n",
    "max_features = [\"sqrt\", \"log2\", None]\n",
    "min_samples_split = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "min_samples_leaf = [1, 2, 3, 4, 5, 7]#Increase leads to increase in bias and decrease in variance\n",
    "#bootstrap = [True, False] always true for oob\n",
    "criterion=['gini', 'entropy']\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               #'bootstrap': bootstrap,\n",
    "               'criterion': criterion}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 200, cv = 3, verbose=2, n_jobs = -1)\n",
    "rf_random.fit(x_train, y_train)\n",
    "rfc = rf_random.best_estimator_\n",
    "\n",
    "#ressults with validation set\n",
    "y_pred = rfc.predict(x_val)\n",
    "print(rf_random.best_estimator_)\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_val, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_val, y_pred)}')\n",
    "print(\"\\n\")\n",
    "\n",
    "#results with actual test set\n",
    "y_real_pred = rfc.predict(x_test)\n",
    "print(\"For actual test set we get:\")\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_real_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_real_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_real_pred)}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = []\n",
    "error_rate = []\n",
    "\n",
    "min_estimators = 10\n",
    "max_estimators = max(n_estimators)\n",
    "\n",
    "for i in range(min_estimators, max_estimators + 1, 1):\n",
    "    rfc.set_params(n_estimators=i)\n",
    "    rfc.fit(x_train, y_train)\n",
    "    oob_error = 1 - rfc.oob_score_\n",
    "    idx.append(i)\n",
    "    error_rate.append(oob_error)\n",
    "\n",
    "\n",
    "plt.plot(idx, error_rate)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Test for over/ underfitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sizes_abs_accuracy, train_accuracy, test_accuracy = learning_curve(estimator=rfc, X=x_val, y=y_val, scoring='accuracy', cv=5, random_state=2)\n",
    "train_sizes_abs_error, train_error, test_error = learning_curve(estimator=rfc, X=x_val, y=y_val, scoring='recall', cv=5, random_state=0)\n",
    "train_avg_accuracy, test_avg_accuracy, train_avg_error, test_avg_error = [], [], [], []\n",
    "\n",
    "\n",
    "for i in range(len(train_sizes_abs_accuracy)):\n",
    "    train_avg_accuracy.append(np.average(train_accuracy[i]))\n",
    "    test_avg_accuracy.append(np.average(test_accuracy[i]))\n",
    "\n",
    "for i in range(len(train_sizes_abs_error)):\n",
    "    train_avg_error.append(np.average(train_error[i]))\n",
    "    test_avg_error.append(np.average(test_error[i]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Accuracy learning curve\n",
    "ax[0].plot(train_sizes_abs_accuracy, train_avg_accuracy)\n",
    "ax[0].plot(train_sizes_abs_accuracy, test_avg_accuracy)\n",
    "ax[0].legend(['Train', 'Validation'])\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Num. samples')\n",
    "\n",
    "# Error learning curve\n",
    "ax[1].plot(train_sizes_abs_error, train_avg_error)\n",
    "ax[1].plot(train_sizes_abs_error, test_avg_error)\n",
    "ax[1].legend(['Train', 'Validation'])\n",
    "ax[1].set_xlabel('Num. of training samples')\n",
    "ax[1].set_ylabel('Recall')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Could plot against test set, but best to do again validation as it shows us how our model changes.\n",
    "\n",
    "The learning curve changes a lot each time. This may be because there isn't much relationship between the variables and label, but we used cross validation to get rid of this issue - another reason may be because our dataset is too small. There is a fairly large gap between my train and validation sets, so I increased the train/test/validation split as I feel the graph could trend towards a more accurate union between the two sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After hyperparameter tuning* and feature selection*\n",
    "y_real_pred = rfc.predict(x_test)\n",
    "auc = roc_auc_score(y_test, y_real_pred)\n",
    "\n",
    "y_pred_prob = rfc.predict_proba(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, metric in enumerate(list(zip(tpr, fpr, thresholds))[1::]):\n",
    "    tpr_, fpr_, threshold =  metric[0], metric[1], metric[2]\n",
    "    print(f'''\n",
    "    TPR: {tpr_}\n",
    "    FPR: {fpr_}\n",
    "    threshold: {threshold}\n",
    "    threshold_idx: {idx + 1}\n",
    "    ''')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to minimise false positives..\n",
    "\n",
    "How does this graph help us? IG I could manually calculate (using the graph) the precesion, accuracy and  recall stats to determine which point is best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dm = DummyClassifier()\n",
    "dm.fit(x_train, y_train)\n",
    "dummy_score = dm.score(x_test, y_test)\n",
    "y_pred = rfc.predict(x_test)  # Model classifications\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'{\"Our model beats the dummy model\" if accuracy > dummy_score else \"Our model does not beat the dummy model.\"}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_real_pred))\n",
    "c = confusion_matrix(y_test, y_pred_real)\n",
    "print(f'True negatives: {c[0][0]}')\n",
    "print(f'False negatives: {c[1][0]}')\n",
    "print(f'True positives: {c[1][1]}')\n",
    "print(f'False positives: {c[0][1]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
